<!DOCTYPE html>
<html lang = 'pt-br'>
    <head>
        <title> Portfólio - Vinícius Saurin</title>
        <meta name = 'description' content = 'Este site tem como objetivo apresentar o portfólio de trabalhos de Vinícius Saurin'>  <!-- Descrição do site -->
        <meta name = 'keywords' content = 'machine-learning machine learning data-science data science'> <!-- Palavras chave para os motores de busca -->
        <meta name = 'robots' content = 'index, follow'> <!-- Google indexa o site -->
        <meta charset= 'utf-8'> <!-- Compreender simbolos do português -->
        <meta name = 'author' content = 'Vinícius Saurin'> <!-- Autor do site -->
        <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
        <link rel="canonical" href="https://www.viniciussaurin.com.br/portfolio_rl_10armedbandit.php">
        <link rel="apple-touch-icon" sizes="180x180" href="/medias/favicon/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/medias/favicon/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/medias/favicon/favicon-16x16.png">
        <link rel="manifest" href="/medias/favicon/site.webmanifest">
        <link rel="mask-icon" href="/medias/favicon/safari-pinned-tab.svg" color="#5bbad5">
        <link rel="shortcut icon" href="/medias/favicon/favicon.ico">
        <meta name="msapplication-TileColor" content="#2b5797">
        <meta name="msapplication-config" content="/medias/favicon/browserconfig.xml">
        <meta name="theme-color" content="#ffffff">
        <link rel = 'stylesheet' href='css/portfolio.css' type = 'text/css'>
        <link rel = 'stylesheet' href='css/fontawesome-free-5.11.2-web/css/all.min.css' type = 'text/css'>
        <link rel = 'stylesheet' href='css/fontawesome-free-5.11.2-web/css/brands.min.css' type = 'text/css'>
        <link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">
        <link rel = 'stylesheet' href='https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css' type = 'text/css'>
        <a name="topo"></a>
        <!-- <script src="https://kit.fontawesome.com/932aed5840.js" crossorigin="anonymous"></script> -->
    </head>
    <body>
            
        <div class="wrapper">
            <nav class="flex-nav">
                <a href="#" class="toggleNav"> ☰ Menu</a>
                <ul>
                    <li><a href="index.php">Home</a></li>
                    <li><a href="portfolio_rl_10armedbandit.php">Portfólio</a></li>
                    <li><a href="sobre.php">Sobre</a></li>
                    <li><a href="contato.php">Contato</a></li>
                    <li class="social">
                        <a href="https://www.linkedin.com/in/vinicius-saurin-594352197/" target="_blank">
                            <i class="fa fa-linkedin">
                            </i>
                        </a>
                    </li>
                    <li class="social">
                            <a href="https://github.com/vinisaurin/" target="_blank">
                                <i class="fa fa-github">
                                </i>
                            </a>
                    </li>
                    <!--
                    <li class="social">
                        <a href="https://www.hackerrank.com/vinisaurin">
                            <i class="fab fa-hackerrank">
                            </i>
                        </a>
                    </li>
                    <li class="social">
                        <a href="https://www.freecodecamp.org/vinisaurin">
                            <i class="fab fa-free-code-camp">
                            </i>
                        </a>
                    </li>
                    <li class="social">
                        <a href="https://www.kaggle.com/vinciussaurin">
                            <i class="fab fa-kaggle">
                            </i>
                        </a>
                    </li>
                    <li class="social">
                        <a href="https://stackoverflow.com/users/12397607/vinisaurin">
                            <i class="fab fa-stack-overflow">
                            </i>
                        </a>
                    </li>
                    -->
                </ul>
            </nav>
        </div>
        
        <div class="hero">
            <div class="with-sidebar">
                <div class="sidebar">
                    <h2>Projetos</h2>
                    <ul>
                        <div class="nav-sidebar">
                            <div class="container-tp container-tp1 open">
                                <div class="container-img">
                                    <img src="medias/datascience.svg" alt="Ícone Data Science">
                                </div>
                                <a href="#" class="toggleNav-sb1 open">Data Science <span>▼</span></a>                 
                            </div>
                            <ul class="sb1 open">
                                <li> <a href="portfolio_crise_diesel.php" class="link-portfolio"> Crise dos combustíveis</a></li>
                                <li> <a href="portfolio_web_scraping_rn.php" class="link-portfolio"> Web Scraping</a></li>
                                <li> <a href="portfolio_seg_pub_rj.php" class="link-portfolio"> Segurança Pública RJ </a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 4 </a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 5 </a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 6 </a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 7 </a></li>
                            </ul>
                            <div class="container-tp container-tp2">
                                <div class="conteiner-img">
                                    <img src="medias/machine-learning.svg" alt="Ícone Machine Learning">
                                </div>
                                <a href="#" class="toggleNav-sb2">Machine Learning<span>▼</span></a>
                            </div>
                            <ul class="sb2">
                                <li> <a href="portfolio_rl_10armedbandit.php" class="link-portfolio"> 10-armed bandit problem </a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 2</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 3</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 4</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 5</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 6</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 7</a></li>
                            </ul>
                            <div class="container-tp container-tp3">
                                <div class="container-img">
                                    <img src="medias/asset-management.svg" alt="Ícone Asset Management">
                                </div>
                                <a href="#" class="toggleNav-sb3">Asset Management<span>▼</span></a>
                            </div>                                
                            <ul class="sb3">
                                <li> <a href="" class="link-portfolio"> Projeto 1</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 2</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 3</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 4</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 5</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 6</a></li>
                                <li> <a href="" class="link-portfolio"> Projeto 7</a></li>
                            </ul>
                        </div>
                    </ul>
                </div>
                <div class="not-sidebar">
                    <h2> Reinforcement Learning: The 10-armed bandit problem</h2>
                    <time class=container-time>
                        <div class="container-img">
                            <img src="medias/calendar.svg" alt="Calendário">
                        </div>    
                        16/01/2020
                    </time>
                    <article>
                        <p>
                            Segundo o wikipedia, Reinforcement Learning é uma área do Machine Learning preocupada com a maneira
                            como um agente (software) deve executar ações em um ambiente para maximizar a expectativa de recompensa
                            acumulada. O Reinforcement Learning é um dos três paradigmas básicos de Machine Learning, além de Supervised
                            Learning e Unsupervised Learning.
                        </p>
                        <p>
                            Entre as principais referências quando falamos do assunto, está o livro "Reinforcement Learning: An introduction",
                            segunda edição, escrito por Richard S. Sutton and Andrew G. Barto. O objetivo deste projeto é reproduzir os exemplos
                            utilizados no livro, assim como resolver os exercícios de programação deixados pelos autores no capítulo 2. Adicionalmente,
                            quando for oportuno serão feitas análises adicionais.
                        </p>
                        <h3>O problema</h3>
                        <p>
                            Imagine o seguinte problema, você se depara repetidamente com uma escolha entre <i>k</i> diferentes opções, ou ações.
                            Após cada escolha você recebe uma recompensa numérica retirada de uma distribuição de probabilidade estacionária que
                            depende da ação escolhida. O seu objetivo é maximizar a recompensa total esperada durante algum período de tempo, por
                            exemplo, durante 1.000 seleções de ação, ou espaços de tempo. Este é exatamente o problema dos <i>k</i> bandidos armados,
                            na tradução literal.
                        </p>
                        <p>
                            Para apresentarmos os modelos apresentados, será necessário introduzir o problema. Abaixo está o gráfico com a distribuição
                            aleatória de cada uma das 10 ações, sendo que a média de cada ação foi retirada de uma distribuição Normal padrão (com média 0
                            e desvio padrão 1), e o desvio padrão é igual a 1.
                            <img src="medias/RL_Multiarmed_bandit/reward_dist.png" alt="Distribuição dos retornos de cada ação">
                        </p>
                        <h3>Método estritamente ganancioso</h3>
                        <p>
                            O primeiro método a ser apresentado é o método "estritamente ganancioso", este método escolhe a ação que possui a maior estimativa
                            de recompensa esperada, sendo que esta estimativa é a média simples das recompensas recebidas até o momento. Nas duas figuras abaixo, são
                            apresentadas a média das recompensas recebidas e o percentual de ações ótimas tomadas em cada passo, com 2000 repetições. Sendo que uma ação é
                            considerada ótima se esta ação possui a distribuição de recompensas com maior média, que no caso é a ação 8.
                            <img src="medias/RL_Multiarmed_bandit/AR_SG.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_SG.png" alt="Percentual de ações ótimas">
                        </p>
                        <h3>Dilema entre investigar e explorar</h3>
                        <p>
                            O método estritamente ganancioso nos apresenta um problema que é o dilema entre investigar e explorar. Dependendo das recompensas que o agente vai
                            recebendo, a estimativa dos retornos esperados se mantém alta, e por tanto o agente não investigará as outras ações para verificar se estas possuem
                            maior recompensa esperada, e somente continuará explorando o seu atual conhecimento. Ao mesmo tempo, caso o agente decida investigar as outras ações, 
                            ele terá que aceitar receber menores retornos para tentar encontrar a ação com maior média.
                        </p>
                        <h3>Método &epsilon;-ganancioso</h3>
                        <p>
                            Uma das formas de tentar contornar o dilema apresentado é o método &epsilon;-ganancioso. Este método impõe que em &epsilon;, numéro entre 0 e 1, das vezes 
                            o agente escolherá aleatóriamente entre todas as ações e em (1 - &epsilon;) das vezes ele continuará a explorar o seu conhecimento, e portanto escolhendo a
                            ação gananciosa. Abaixo estão apresentados os mesmos gráficos apresentados anteriormente, mas com a aplicação do método &epsilon;-ganancioso, com &epsilon;
                            igual a 0.10 e 0.03, respectivamente.
                            <img src="medias/RL_Multiarmed_bandit/AR_EG10.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_EG10.png" alt="Percentual de ações ótimas">
                            
                        </p>
                        <p>
                            Perceba que no caso &epsilon; = 0.03, o gráfico de recompensa média atinge um patamar ligeiramente maior que no outro caso, entretanto, assim como no gráfico
                            de percentual de ações ótimas, demora-se muito mais para atingir o ponto em que a curva se torna quase <i>flat</i>. Enquanto que para o &epsilon; = 0.10, o gráfico
                            fica achatado por volta do passo 600, quando &epsilon;=0.03 a curva só se torna <i>flat</i> por volta do passo 900.

                            <img src="medias/RL_Multiarmed_bandit/AR_EG03.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_EG03.png" alt="Percentual de ações ótimas">
                        </p>
                        <h3>Método de valores iniciais otimistas</h3>
                        <p>
                            Valores iniciais podem ser usados, de uma forma simples, para encorajar a investigação. Suponha que sejam configurados os valores iniciais como iguais a 5, para o problema
                            que estamos analisando com distribuição de recompensa em torno de 0, este com certeza é um número otimista. Conforme as ações vão sendo escolhidas, com probabilidade muito
                            alta o retorno delas não será maior que 5, portanto o agente se "sentirá desapontado" e procurará uma outra ação até a estimativa de todas as ações convergir para o valor
                            real. Abaixo estão os gráficos com a reprodução do mesmo explo para o método de valores iniciais otimistas.
                            <img src="medias/RL_Multiarmed_bandit/AR_OIV.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_OIV.png" alt="Percentual de ações ótimas">

                        </p>
                        <h3>Método Upper-Confidence-Bound</h3>
                        <p>
                            No método &epsilon;-ganancioso, quando o agente "resolve" investigar, ele escolhe aleatóriamente entre todas as ações, mas não seria muito melhor caso fosse colocado algum
                            critério de escolha para a exploração? Uma maneira muito efetiva de fazer isso é escolher a ação com a maior banda superior do intervalo de confiança, ou seja, o agente
                            escolherá a ação que apesar de pouco conhecida, tenha o maior potencial de ter uma média de retornos superior às outras. Perceba que este método consegue atingir níveis superiores
                            aos outros métodos tanto da média das recompensas, quanto do percentual de ações ótimas.
                            <img src="medias/RL_Multiarmed_bandit/AR_UCB.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_UCB.png" alt="Percentual de ações ótimas">
                        </p>
                        <h3>Algoritmos Gradient Bandit</h3>
                        <p>
                            Até agora foram considerados apenas métodos que fazem a seleção de ações de acordo com a estimativa da recompensa esperada. Entretando, os algoritmos <i>Gradient Bandit</i>
                            utilizam uma abordagem diferente, este método atribui a cada ação uma preferência númerica, segundo a qual será atribuída uma probabilidade de cada ação ser escolhida a cada 
                            passo, de acordo com a distribuição <i>soft-max</i> ou distribuição de <i>Gibbs or Boltzmann</i>. 
                            <img src="medias/RL_Multiarmed_bandit/AR_GBA.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_GBA.png" alt="Percentual de ações ótimas">
                        </p>
                        <p>
                            Finalmente, abaixo estão apresentados os gráficos comparativos de todos os métodos apresentados até agora. Repare que o método estritamente ganancioso é muito inferior aos demais,
                            da mesma forma que os métodos &epsilon;-ganancioso e valores iniciais otimistas possuem resultados muito parecidos, com a diferença de que o último atinge um ponto ótimo
                            bem mais rápido do que os outros. Os métodos com maior superioridade para o problema dos k-bandidos armados são o <i>upper-confidence-bound</i> e algoritimos <i>Gradient Bandit</i>. 
                            Vale ressaltar que nem sempre estes dois métodos serão superiores aos demais, tudo depende de qual problema é apresentado, pois cada método possui seus pontos fracos e fortes.
                            <img src="medias/RL_Multiarmed_bandit/AR_CM.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_CM.png" alt="Percentual de ações ótimas">
                        </p>
                        <h3>Problema da não estacionariedade</h3>
                        <p>
                            Até o momento foi abordado apenas o problema estacionário, no qual a distribuição das recompensas se mantém constante durante todo o tempo. Contudo, na vida real é muito mais provável
                            que se enfrente problemas não estacionários e nestes casos o método &epsilon;-ganancioso, o qual utiliza da média simples, tem uma queda brusca de performance. Para provar isso, 
                            utilizou-se de uma variação do método, na qual é utilizada uma média ponderada, atribuindo maiores pesos aos retornos mais recentes e menores pesos aos retornos mais antigos. Neste teste, 
                            as distribuições das recompensas começarão com a média 0, mas a cada passo será adicionada uma variável aleatória com distribuição Normal com média 0 e desvio-padrão 0.1. Testou-se 200 
                            repetições para 10.000 passos.
                            <img src="medias/RL_Multiarmed_bandit/AR_CM_10K.png" alt="Média das recompensas">
                            <img src="medias/RL_Multiarmed_bandit/OA_CM_10K.png" alt="Percentual de ações ótimas">
                        </p>
                        <p>
                            Estes foram os métodos apresentados no capítulo 2 do livro referência, portanto são os métodos considerados mais importantes e efetivos pelos autores. Todavia, o problema dos k-bandidos 
                            armados é um problema muito simples e apenas introdutório para o campo do <i>Reinforcement Learning</i>. Posteriormente, serão postadas mais análises e replicações dos métodos mais avançados
                            explicados neste livro.
                        </p>
                        
                    </article>
                    <a href="#topo" id="BackToTop">Voltar ao topo</a>
                    <br>
                    <br>
                    <span id="github-ind">*Todos os projetos desenvolvidos nesse site possuem os códigos fontes e resultados armazenados no <a href="https://github.com/vinisaurin/RL_Multi-Armed_Bandits_Problem" target="_blank">github <i class="fa fa-github"></i></a>.</span>
                </div>
            </div>
        </div>
        <footer>
            <p> Vinícius Saurin &copy; 2020</p>
        </footer>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
        <script>
            $(function() {
                $('.toggleNav').on('click', function(){
                    $('.flex-nav ul').toggleClass('open');
                    // $('.flex-nav ul').slideToggle();
                });
            });
            // $(function() {
            //     $('.toggleNav-sb1').on('click', function(){
            //         $('.nav-sidebar .sb1').toggleClass('open');
            //         $('.toggleNav-sb1').toggleClass('open');
            //         // $('.flex-nav ul').slideToggle();
            //     });
            // });
            // $(function() {
            //     $('.toggleNav-sb2').on('click', function(){
            //         $('.toggleNav-sb2').toggleClass('open');
            //         $('.nav-sidebar .sb2').toggleClass('open');
            //         // $('.flex-nav ul').slideToggle();
            //     });
            // });
            // $(function() {
            //     $('.toggleNav-sb3').on('click', function(){
            //         $('.nav-sidebar .sb3').toggleClass('open');
            //         $('.toggleNav-sb3').toggleClass('open');
            //         // $('.flex-nav ul').slideToggle();
            //     });
            // });
            $(function() {
                $('.container-tp1').on('click', function(){
                    $('.nav-sidebar .sb1').toggleClass('open');
                    $('.container-tp1').toggleClass('open');
                    // $('.flex-nav ul').slideToggle();
                });
            });
            $(function() {
                $('.container-tp2').on('click', function(){
                    $('.nav-sidebar .sb2').toggleClass('open');
                    $('.container-tp2').toggleClass('open');
                    // $('.flex-nav ul').slideToggle();
                });
            });
            $(function() {
                $('.container-tp3').on('click', function(){
                    $('.nav-sidebar .sb3').toggleClass('open');
                    $('.container-tp3').toggleClass('open');
                    // $('.flex-nav ul').slideToggle();
                });
            });
        </script>
    </body>
</html>